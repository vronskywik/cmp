{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a pipeline to process stock loan data from several prime brokers where the datasets consist of columnar data (date, stockid, broker, measure_one, measure_two). The zip file from the broker is called 'dc-test-file-ingestion.zip' and you can assume it is in the FILEPATH (confirm by os.listdir) if you'd like). Design a data schema for storing this data, including any raw and derived tables you would like to use. Clean as many data errors as you can find so that the table is useful for machine learning.\n",
    "\n",
    "Document your assumptions in the code. Your answer should in the form of a python function.\n",
    "\n",
    "Finally, in words, describe a robust ingestion pipeline that can handle such messy data files, the infrastructure you would need to run this pipeline. What are the possible \"real world\" scenarios that you need to handle which are outside of the \"academic\" scenario where clean data comes daily? How would you detect these cases and handle them? Does this list match the data issues you see in the provided sample data? Include this as a comment block in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: measure_one_dates (2828043505.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    measure_one_dates=('date', 'max')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated: measure_one_dates\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTES\n",
    "- The basic data cleaning methods are only processed here. More sophisticated measures like checking for missing data using metadata & looking at spikes, confirming file structures\n",
    "and data types should be added once the nuances of the data sets are understood.\n",
    "- Additional derives stats should include: standard deviation of the measures, a count or display of the distinct stockid represented over time, a count or display of the\n",
    "distinct broker represented over time.\n",
    "- In response to describing a robust ingestion pipeline and how to handle different scenarios, I have added my answer to Q6, data monitoring as this covers the same aspects here.\n",
    "\n",
    "ASSUMPTIONS\n",
    "- apart from dropping NA & removing duplicates, we assume the data is clean, timely and correct.\n",
    "- The zip file contains only one CSV file, and its name is unknown.\n",
    "- We assume the CSV file is present in the zip archive.\n",
    "- Assume the 'date' column is in datetime format, and 'stockid' and 'broker' columns are strings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "def process_stock_loan_data(filepath='FILEPATH'):\n",
    "    # Confirm the file names in the zip archive using os.listdir\n",
    "    with ZipFile(os.path.join(filepath, 'dc-test-file-ingestion.zip'), 'r') as zip_ref:\n",
    "        file_names = zip_ref.namelist()\n",
    "\n",
    "    # Load the CSV file into a Pandas DataFrame\n",
    "    with ZipFile(os.path.join(filepath, 'dc-test-file-ingestion.zip'), 'r') as zip_ref:\n",
    "        with zip_ref.open(file_names[0]) as file:\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "    # Data Cleaning and Transformation\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['stockid'] = df['stockid'].astype(str) \n",
    "    df['broker'] = df['broker'].astype(str)\n",
    "\n",
    "    # Handle for missing values by dropping rows with any NaN values.\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Handle for duplicates by keeping the first occurrence.\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Schema: store the raw data in a table named 'raw_data'\n",
    "    raw_data_table = df.copy()\n",
    "\n",
    "    # Create a derived table that looks at the dates available per measure, you can duplicate this for measure two as well. \n",
    "    derived_measure_one_dates = raw_data_table.groupby(['measure_one','date']).agg(\n",
    "        measure_one_dates=('date', 'min'),\n",
    "        measure_one_dates=('date', 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Create a derived table that reports on each measures basis stats\n",
    "    derived_measures_stats = raw_data_table.groupby(['date', 'stockid', 'broker']).agg(\n",
    "        measure_one_mean=('measure_one', 'mean'),\n",
    "        measure_one_mean=('measure_one', 'min'),\n",
    "        measure_one_mean=('measure_one', 'max'),\n",
    "        measure_two_mean=('measure_one', 'mean'),\n",
    "        measure_two_mean=('measure_one', 'min'),\n",
    "        measure_two_mean=('measure_one', 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Return the processed data\n",
    "    return raw_data_table, derived_measures_stats, derived_measure_one_dates\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "0. Use case\n",
    "\n",
    "- What: Why is the data being procured\n",
    "- Action: Understand the use case, so you can work backward to satisfy this at a minimum in the data lifecycle / pipeline setup. \n",
    "\n",
    "1. Sources\n",
    "\n",
    "- What: Different Vendor/source level qualitative nuances, data summary. \n",
    "- Actions: Understand the competitive advantages, use cases, proof of concept studies and limitations of the different data vendors coming in from a qualitative \n",
    "perspective (including choice for symbology, coverage, mapping, any third party licensing requirements, ddq' etc).\n",
    "\n",
    "2. Structure\n",
    "\n",
    "- What: How is the data stored? Checking for file & scherma changes sytematically. Is it relational? Would multiple tables be required to form a complete update cycle? \n",
    "What is the format? e.g. parquet, json, csv etc.\n",
    "- Actions: Bucket each data product inbound by the structures offered, understand if they are the best options offered per vendor (based on alternatives from \n",
    "the vendor and any third parties). Find a balance of opting for an optimized number of structures upstream to deal with. Are there any advantages or disadvantages \n",
    "in taking one type of structure per data product vs the alternatives offered? Look at utalising auto schema detection libraries or build one. Simple steps are crucial like \n",
    "checking for column count changes, column name changes, data type changes, deviation of the size of updates coming through\n",
    "\n",
    "3. Delivery\n",
    "\n",
    "- What: How is the data being distributed? eg files, sql/oracle updates, rest api, snowflake? Another proprietary third party \n",
    "- Actions: Optimize which delivery mechanism per data set provides a balance between timeliness, engineering resource requirements downstream, least IT/Infrastructure challenges, \n",
    "most secure based on company standards. If only a subset of the data is required, is there a smart api that can be used instead of taking receipt of the entire data set everyday. \n",
    "This should be use case based unless you can afford to take the entire dataset and store it (which may make sense in some instances where a data vendor charges extra to revisit history)\n",
    "\n",
    "4. Reference & Metadata\n",
    "\n",
    "- What: What reference data, metadata, symbology (id's) and any other complimentary data does each dataset offer\n",
    "- Actions: Build the security master and mapping system to accommodate the collective reference data / symbology provided by each of the data sets so that \n",
    "upstream use cases across multiple datasets is possible & effective. \n",
    "\n",
    "5. Storage\n",
    "\n",
    "- What: How will the data be stored? Will there be multiple levels of storage required based on different use cases? Does it comply with internal data governance? \n",
    "- Actions: Balance the different stages of data storage from raw -> data warehouse -> data lake. What users will be accessing from each level? Do you need a to impliment a\n",
    "less frequently updated mirror for disaster recovery purposes? Ensure processes and storage upkeep data integrity, confidentiality, and compliance with data protection regulations. \n",
    "I am happy to elaborate on how I have staged data in past data lake projects. \n",
    "\n",
    "6. Updates\n",
    "\n",
    "- What: How often does the dataset update per data product.\n",
    "- Actions: Understand how often appends can happen, how often corrections can happen and how to accommodate for both in a timely manner. \n",
    "Understand what is the most optimal update frequency to satisfy the use case/s. Build a mechanism to take only new data rather than loading all the data everytime. Importantly, to preserve point in time,\n",
    "keep historical corrections of data.\n",
    "\n",
    "7. Sequencing\n",
    "\n",
    "- What: From a set of data products, do some updates need to precede others? \n",
    "- Actions: Here we are looking to build pipelines with independent data sets at the front of the queue (eg security master, corporate actions, symbology,\n",
    "instrument reference) and then followed by data sets that come to stick onto the security master system based on priority of importance driven by the \n",
    "analytics needs real time vs delayed reporting downstream. You should also invest in data orchestration tools to help manage sequencing and alerting of updates (eg apache airflow).\n",
    "\n",
    "8. Quality & Remediation\n",
    "\n",
    "- What: Making sure the timeliness, completeness & technical aspects are checked on any new data update.\n",
    "- Actions: Start with basic technical data checks (types, changes to schema, missing data etc), then move to data family specific tests (e.g. spikes for time series data), \n",
    "then more sophisticated tests based on the data set (specific nuances). The checks will take longer on the initial history build and then apply systematic alerts prioritized by \n",
    "importance of upstream needs. Alerts can be in real time and or email -like alerts. Following this, there should be tools and documentation to help remediation. \n",
    "\n",
    "9. Upstream delivery\n",
    "\n",
    "- What: What are the internal upstream delivery mechanisms (api vs direct analytics)? Are permissions put in place?\n",
    "- Action: Build out of a different api's to allow multiple user types to discover, interact with and use the data. Making sure the right permissions are in place based on \n",
    "internal regulation and the data usage licensing. \n",
    "\n",
    "10. Consumer review\n",
    "\n",
    "- What: Initial understanding and continual review of upstream use for data.\n",
    "- Actions: It's key to review the use cases your data & pipelines are feeding, from improvements to timeliness, coverage and quality, continually working with \n",
    "upstream can help optimize many of the facets discussed here. \n",
    "\n",
    "11. Vendor review\n",
    "\n",
    "- What: Initial understanding (independent and in conjunction with the data vendors product team) & continual review of the source data.\n",
    "- Actions: The independent review of data should be done first without any biases to understand how the data behaves and how it updates, you can also connect with \n",
    "the data vendors product team following that to understand their SLA's on the data and delivery, continually proactive review of the source data will help you keep on top \n",
    "of changes to the data which may affect the pipeline process. Sign up to alerts for changes in the data published by the source but also do your own systematic checks independently.\n",
    "\n",
    "12. Statistics\n",
    "\n",
    "- What: Understanding data update patterns and changes over time\n",
    "- Actions: Tools and libraries to help run periodic tests on data sets and update logs to understand SLA's on data. Useful for audits & commercial negotiations during contract renewals. Logging \n",
    "methods shoudl be robust to include errors or anomalies during the lifecycle of the data.\n",
    "\n",
    "13. Documentation\n",
    "\n",
    "- What: Notes on pipeline processes. \n",
    "- Actions: Create accessible Well typed, standardized & defined notes on pipelines & remediation. Make sure they are updated on a regular basis.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
